# UnimodalLanguageRepresentations Language Modeling – Exercise 1 (Group 11)

This repository contains our solutions for **Exercise 1** of the course on Unimodal Language Representations at TU Darmstadt.

## 📂 Contents
- `E1.pdf` – Original exercise sheet with task description  
- `E1_11.pdf` – Our written group report (Group 11)  
- `E1_11.ipynb` – Google Colab notebook with code and experiments  

## 👥 Authors
This work was completed as a **group assignment** by:  
- Viktor Maximilian Lehnhausen  
- Mikael Alves Brito  
- Georg Matthes  

## 📝 Description
The exercise focused on:
- Tokenization strategies (character-level vs. subword tokenization)  
- Implementing Transformer blocks with masked self-attention  
- Implementing **Sinusoidal Position Embeddings** (Vaswani et al. 2017)  
- Training and evaluating language models on *The Lord of the Rings* dataset  
- Comparing perplexity and generative ability with different configurations  

## ⚠️ Note
This project was created as part of a **university exercise sheet** and is **not individual work**.  
All results were prepared collaboratively by Group 11.  
