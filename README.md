# UnimodalLanguageRepresentations Language Modeling â€“ Exercise 1 (Group 11)

This repository contains our solutions for **Exercise 1** of the course on Unimodal Language Representations at TU Darmstadt.

## ğŸ“‚ Contents
- `E1.pdf` â€“ Original exercise sheet with task description  
- `E1_11.pdf` â€“ Our written group report (Group 11)  
- `E1_11.ipynb` â€“ Google Colab notebook with code and experiments  

## ğŸ‘¥ Authors
This work was completed as a **group assignment** by:  
- Viktor Maximilian Lehnhausen  
- Mikael Alves Brito  
- Georg Matthes  

## ğŸ“ Description
The exercise focused on:
- Tokenization strategies (character-level vs. subword tokenization)  
- Implementing Transformer blocks with masked self-attention  
- Implementing **Sinusoidal Position Embeddings** (Vaswani et al. 2017)  
- Training and evaluating language models on *The Lord of the Rings* dataset  
- Comparing perplexity and generative ability with different configurations  

## âš ï¸ Note
This project was created as part of a **university exercise sheet** and is **not individual work**.  
All results were prepared collaboratively by Group 11.  
